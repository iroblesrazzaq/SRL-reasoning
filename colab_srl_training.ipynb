{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0061fd88",
      "metadata": {},
      "source": [
        "# SRL GRPO Training (Colab, A100)\n",
        "End-to-end notebook to build SRL data, split 95/5, and train with GRPO + LoRA on an A100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15f69b73",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 0. Environment setup (clone + installs)\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = \"https://github.com/iroblesrazzaq/SRL-reasoning.git\"\n",
        "REPO_DIR = Path('/content/SRL-reasoning')\n",
        "REPO_DIR_STR = str(REPO_DIR)\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "    !git clone $REPO_URL $REPO_DIR_STR\n",
        "\n",
        "os.chdir(REPO_DIR_STR)\n",
        "if REPO_DIR_STR not in sys.path:\n",
        "    sys.path.append(REPO_DIR_STR)\n",
        "\n",
        "!pip install -q transformers peft bitsandbytes accelerate datasets trl vllm flash-attn --no-build-isolation\n",
        "!pip install -q git+https://github.com/huggingface/trl.git\n",
        "!pip install -q -e .\n",
        "\n",
        "import torch\n",
        "device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
        "print(f'PyTorch {torch.__version__} | Device: {device_name}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c11e5a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 1. Global config\n",
        "import random, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "SEED = 42\n",
        "BASE_MODEL = 'Qwen/Qwen2.5-7B-Instruct'\n",
        "OUTPUT_DIR = Path('/content/outputs/srl_grpo')\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "DATA_DIR = REPO_DIR / 'data'\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "print('DATA_DIR:', DATA_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b515ef21",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 2. Build SRL data (s1K-1.1 -> step-wise JSONL)\n",
        "from src.shared.build_srl_data import load_teacher_dataset, normalize_dataset, build_srl_dataset, save_jsonl\n",
        "from src.shared.splits import split_by_trajectory\n",
        "\n",
        "raw_ds = load_teacher_dataset('simplescaling/s1K-1.1', split='train')\n",
        "norm_trajs = normalize_dataset(raw_ds)\n",
        "srl_examples = build_srl_dataset(norm_trajs)\n",
        "\n",
        "all_path = DATA_DIR / 'srl_steps.jsonl'\n",
        "save_jsonl(srl_examples, all_path)\n",
        "\n",
        "train_examples, val_examples, _ = split_by_trajectory(\n",
        "    str(all_path),\n",
        "    train_ratio=0.95,\n",
        "    val_ratio=0.05,\n",
        "    test_ratio=0.0,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "train_path = DATA_DIR / 'train.jsonl'\n",
        "val_path = DATA_DIR / 'val.jsonl'\n",
        "save_jsonl(train_examples, train_path)\n",
        "save_jsonl(val_examples, val_path)\n",
        "\n",
        "print(f'Train examples: {len(train_examples)}')\n",
        "print(f'Val examples:   {len(val_examples)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f0359ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 3. Prepare HF datasets for GRPO\n",
        "from scripts.train_srl import load_srl_dataset\n",
        "\n",
        "train_dataset = load_srl_dataset(str(train_path))\n",
        "val_dataset = load_srl_dataset(str(val_path))\n",
        "\n",
        "print(train_dataset[:2])\n",
        "print(f'HF datasets -> train {len(train_dataset)}, val {len(val_dataset)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f513bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 4. Load model + tokenizer (LoRA, flash-attn, grad checkpointing)\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    padding_side='left',\n",
        "    trust_remote_code=True,\n",
        " )\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    attn_implementation='flash_attention_2',\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules='all-linear',\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    bias='none',\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.enable_input_require_grads()\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'Trainable params: {trainable_params/1e6:.1f}M / {total_params/1e6:.1f}M')\n",
        "if torch.cuda.is_available():\n",
        "    print('Model device:', next(model.parameters()).device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b341df51",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 5. Configure GRPO trainer and train\n",
        "from inspect import signature\n",
        "from trl import GRPOConfig\n",
        "from scripts.train_srl import SRLGRPOTrainer, create_reward_function\n",
        "\n",
        "reward_fn = create_reward_function(tokenizer)\n",
        "\n",
        "grpo_kwargs = {\n",
        "    'output_dir': str(OUTPUT_DIR),\n",
        "    'num_train_epochs': 30,             # [cite: 235] Matches paper\n",
        "    'per_device_train_batch_size': 8,   # A100 80GB capacity\n",
        "    # To match Paper Batch Size of 512: 8 * 64 = 512\n",
        "    'gradient_accumulation_steps': 64,  # [cite: 523]\n",
        "    'learning_rate': 5e-7,              # [cite: 530] Paper uses 5e-7\n",
        "    'beta': 0.0,                        # [cite: 536] KL coeff is 0 for SRL\n",
        "    'warmup_ratio': 0.0,                # [cite: 531] No warmup\n",
        "    'max_grad_norm': 1.0,               # [cite: 525]\n",
        "    'num_generations': 8,               # [cite: 534]\n",
        "    'temperature': 1.0,                 # [cite: 533] Explicitly set rollout temp\n",
        "    \n",
        "    # Convention / Engineering settings\n",
        "    'logging_steps': 1,\n",
        "    'save_strategy': 'epoch',\n",
        "    'evaluation_strategy': 'epoch',     # Paper evaluates on val set\n",
        "    'save_total_limit': 2,\n",
        "    'load_best_model_at_end': True,     # [cite: 235] \"select checkpoint with best perf\"\n",
        "    'metric_for_best_model': 'reward',\n",
        "    'greater_is_better': True,\n",
        "    'optim': 'adamw_8bit',              # Convention (Paper uses H100s, likely 8bit or full)\n",
        "    'bf16': True,                       # [cite: 527]\n",
        "    'report_to': 'none',\n",
        "    'seed': SEED,\n",
        "}\n",
        "\n",
        "supported = set(signature(GRPOConfig.__init__).parameters)\n",
        "grpo_config = GRPOConfig(**{k: v for k, v in grpo_kwargs.items() if k in supported})\n",
        "\n",
        "trainer = SRLGRPOTrainer(\n",
        "    model=model,\n",
        "    args=grpo_config,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    reward_funcs=reward_fn,\n",
        "    filter_epsilon=1e-4,\n",
        ")\n",
        "\n",
        "train_result = trainer.train()\n",
        "print(train_result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff31ede1",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 6. Save best model to Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BEST_DIR = Path('/content/drive/MyDrive/SRL_Best_Model')\n",
        "BEST_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "trainer.save_model(str(BEST_DIR))\n",
        "tokenizer.save_pretrained(str(BEST_DIR))\n",
        "\n",
        "print('Saved best model to', BEST_DIR)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
