{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRL GRPO Training (Colab, A100)\n",
    "\n",
    "Steps: clone repo, install (editable), create train/val split (95/5), train with GRPO + LoRA + gradient checkpointing, evaluate checkpoints on val, and save the best model (optionally to Drive).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab/A100 setup + Drive mount (optional)\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Set how much VRAM vLLM can reserve (leave room for training)\n",
    "os.environ.setdefault(\"VLLM_GPU_MEMORY_UTILIZATION\", \"0.4\")\n",
    "\n",
    "MOUNT_DRIVE = True  # set False if you don't want Drive\n",
    "if MOUNT_DRIVE:\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo and install editable\n",
    "import os, subprocess, textwrap\n",
    "\n",
    "REPO_URL = \"https://github.com/ismaelrazzaq/SRL-reasoning.git\"  # update if different\n",
    "WORKDIR = \"/content/SRL-reasoning\"\n",
    "\n",
    "if not os.path.exists(WORKDIR):\n",
    "    !git clone {REPO_URL} {WORKDIR}\n",
    "else:\n",
    "    %cd {WORKDIR}\n",
    "    !git pull\n",
    "\n",
    "%cd {WORKDIR}\n",
    "\n",
    "# Install deps (includes TRL from source, bitsandbytes, peft)\n",
    "!pip install -U pip\n",
    "!pip install -e .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep\n",
    "- Place your full SRL JSONL at `data/srl_steps.jsonl` (traj_id/problem/previous_steps/step_title/step_body).\n",
    "- This cell splits 95/5 train/val (test 0) by trajectory ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.shared.splits import split_by_trajectory, save_splits\n",
    "\n",
    "DATA_PATH = Path(\"data/srl_steps.jsonl\")  # upload/replace as needed\n",
    "assert DATA_PATH.exists(), f\"Missing {DATA_PATH}, upload your data first.\"\n",
    "\n",
    "train, val, _ = split_by_trajectory(\n",
    "    str(DATA_PATH), train_ratio=0.95, val_ratio=0.05, test_ratio=0.0, seed=42\n",
    ")\n",
    "save_splits(train, val, [], DATA_PATH.parent)\n",
    "print(f\"Train examples: {len(train)} | Val examples: {len(val)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GRPO (LoRA, gradient checkpointing, flash attention)\n",
    "- Saves a checkpoint each epoch under `outputs/srl_model/`.\n",
    "- Adjust `num_train_epochs`, batch sizes, and `max_new_tokens` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%cd {WORKDIR}\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"  # change if desired\n",
    "OUTPUT_DIR = \"outputs/srl_model\"\n",
    "TRAIN_PATH = \"data/train.jsonl\"\n",
    "\n",
    "train_cmd = [\n",
    "    \"python\",\n",
    "    \"scripts/train_srl.py\",\n",
    "    \"--data_path\", TRAIN_PATH,\n",
    "    \"--model_name\", MODEL_NAME,\n",
    "    \"--output_dir\", OUTPUT_DIR,\n",
    "    \"--num_train_epochs\", \"2\",\n",
    "    \"--per_device_train_batch_size\", \"1\",\n",
    "    \"--gradient_accumulation_steps\", \"16\",\n",
    "    \"--learning_rate\", \"1e-6\",\n",
    "    \"--num_generations\", \"8\",\n",
    "    \"--max_new_tokens\", \"256\",\n",
    "    \"--logging_steps\", \"10\",\n",
    "    \"--save_strategy\", \"epoch\",\n",
    "    \"--save_total_limit\", \"4\",\n",
    "    \"--optim\", \"adamw_8bit\",\n",
    "    \"--bf16\",\n",
    "]\n",
    "\n",
    "print(\"Running:\", \" \".join(train_cmd))\n",
    "os.environ.setdefault(\"VLLM_GPU_MEMORY_UTILIZATION\", \"0.4\")\n",
    "! {' '.join(train_cmd)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate checkpoints on validation split\n",
    "- Uses `compute_srl_reward` as a proxy metric (average over val).\n",
    "- Picks best checkpoint and optionally copies to Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from src.shared.prompts import build_srl_prompt, STOP_TOKENS\n",
    "from src.srl.rewards import compute_srl_reward\n",
    "\n",
    "VAL_PATH = Path(\"data/val.jsonl\")\n",
    "assert VAL_PATH.exists(), \"Val split missing.\"\n",
    "\n",
    "def load_val_examples(path, max_examples=None):\n",
    "    out = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_examples and i >= max_examples:\n",
    "                break\n",
    "            if line.strip():\n",
    "                out.append(json.loads(line))\n",
    "    return out\n",
    "\n",
    "def eval_checkpoint(ckpt_path, examples, max_new_tokens=256):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_path, trust_remote_code=True, padding_side=\"left\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = PeftModelForCausalLM.from_pretrained(\n",
    "        ckpt_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    rewards = []\n",
    "    for ex in tqdm(examples, desc=f\"Eval {Path(ckpt_path).name}\", total=len(examples)):\n",
    "        prompt = build_srl_prompt(ex[\"problem\"], ex.get(\"previous_steps\", []), step_title=ex.get(\"step_title\"), include_closing_tag=False)\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt").input_ids.to(model.device)\n",
    "        with torch.no_grad():\n",
    "            gen_ids = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )[0]\n",
    "        full_output = tokenizer.decode(gen_ids, skip_special_tokens=False)\n",
    "        reward = compute_srl_reward(full_output, ex[\"step_body\"])\n",
    "        rewards.append(reward)\n",
    "    return sum(rewards) / len(rewards)\n",
    "\n",
    "val_examples = load_val_examples(VAL_PATH)\n",
    "ckpts = sorted(glob.glob(f\"{OUTPUT_DIR}/checkpoint-*\"))\n",
    "assert ckpts, \"No checkpoints found. Ensure training saved checkpoints.\"\n",
    "\n",
    "scores = []\n",
    "for ckpt in ckpts:\n",
    "    avg_reward = eval_checkpoint(ckpt, val_examples)\n",
    "    scores.append((avg_reward, ckpt))\n",
    "    print(f\"Checkpoint {ckpt} avg reward: {avg_reward:.4f}\")\n",
    "\n",
    "best_score, best_ckpt = max(scores, key=lambda x: x[0])\n",
    "print(f\"Best checkpoint: {best_ckpt} (avg reward {best_score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save best checkpoint to Drive (optional)\n",
    "- Copies the best checkpoint folder into Drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if MOUNT_DRIVE:\n",
    "    drive_dest = f\"/content/drive/MyDrive/srl_best_checkpoint\"\n",
    "    if os.path.exists(drive_dest):\n",
    "        shutil.rmtree(drive_dest)\n",
    "    shutil.copytree(best_ckpt, drive_dest)\n",
    "    print(f\"Copied {best_ckpt} -> {drive_dest}\")\n",
    "else:\n",
    "    print(\"Drive not mounted; skip copy.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
