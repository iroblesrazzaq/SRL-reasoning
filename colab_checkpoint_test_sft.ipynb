{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT Model Format Verification\n",
    "\n",
    "Test the merged SFT model to verify it follows the `<think>...</think>` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Drive & Find Model Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Shared with me\" folders don't appear directly in Colab.\n",
    "# You need to add a shortcut to your Drive first:\n",
    "#\n",
    "# 1. Go to drive.google.com\n",
    "# 2. Click \"Shared with me\" in the left sidebar\n",
    "# 3. Right-click on \"srl_outputs\" folder\n",
    "# 4. Click \"Organize\" → \"Add shortcut\" → \"My Drive\"\n",
    "#\n",
    "# Then the path will be: /content/drive/MyDrive/srl_outputs/merged_sft_4b\n",
    "\n",
    "import os\n",
    "\n",
    "# Try these paths in order\n",
    "possible_paths = [\n",
    "    \"/content/drive/MyDrive/srl_outputs/merged_sft_4b\",\n",
    "    \"/content/drive/My Drive/srl_outputs/merged_sft_4b\",\n",
    "    \"/content/drive/Shareddrives/srl_outputs/merged_sft_4b\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        MODEL_PATH = path\n",
    "        print(f\"✓ Found model at: {MODEL_PATH}\")\n",
    "        break\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    print(\"❌ Model not found at expected paths.\")\n",
    "    print(\"\\nLet's search your Drive...\\n\")\n",
    "    \n",
    "    # Show what's in MyDrive\n",
    "    mydrive = \"/content/drive/MyDrive\"\n",
    "    if os.path.exists(mydrive):\n",
    "        print(f\"Contents of {mydrive}:\")\n",
    "        for item in sorted(os.listdir(mydrive))[:20]:\n",
    "            print(f\"  {item}\")\n",
    "        if len(os.listdir(mydrive)) > 20:\n",
    "            print(\"  ... (more files)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not found above, set the path manually here after adding the shortcut:\n",
    "# MODEL_PATH = \"/content/drive/MyDrive/srl_outputs/merged_sft_4b\"\n",
    "\n",
    "# Verify contents\n",
    "if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
    "    print(f\"Model directory contents:\")\n",
    "    for item in sorted(os.listdir(MODEL_PATH)):\n",
    "        print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"Loading from: {MODEL_PATH}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"✓ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Model loaded on: {next(model.parameters()).device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt & Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "\n",
    "PROMPT_PREAMBLE = (\n",
    "    \"You are a helpful assistant for solving mathematical problems. \"\n",
    "    \"A user will provide a math problem, which may include a partial solution. \"\n",
    "    \"Your task is to continue the solution by providing the very next logical step. \"\n",
    "    \"A user will ask you to solve a task. You should first draft your thinking process \"\n",
    "    \"(inner monologue). Then, generate the solution. \"\n",
    "    \"Your response format must follow the template below:\\n\"\n",
    "    \"<think> Your thoughts or/and draft, like working through an exercise on scratch paper. \"\n",
    "    \"Be as casual and as long as you want until you are confident to generate a correct solution. </think>\\n\"\n",
    "    \"Provide only the single, next step to continue the solution. Do not solve the entire problem.\"\n",
    ")\n",
    "\n",
    "def build_prompt(problem: str, previous_steps: List[str] = None) -> str:\n",
    "    if previous_steps is None:\n",
    "        previous_steps = []\n",
    "    parts = [PROMPT_PREAMBLE, \"\", \"Problem:\", problem.strip(), \"\"]\n",
    "    parts.extend([s.strip() for s in previous_steps])\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def parse_output(text: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Returns (thought, action) or (None, None) if invalid format.\"\"\"\n",
    "    if not text:\n",
    "        return (None, None)\n",
    "    idx = text.find(\"</think>\")\n",
    "    if idx == -1:\n",
    "        return (None, None)\n",
    "    action = text[idx + 8:].strip()\n",
    "    open_idx = text.find(\"<think>\")\n",
    "    thought = text[open_idx + 7:idx].strip() if open_idx != -1 else text[:idx].strip()\n",
    "    return (thought, action)\n",
    "\n",
    "print(\"✓ Functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(problem: str, previous_steps=None, max_tokens=512, greedy=True, verbose=True):\n",
    "    prompt = build_prompt(problem, previous_steps or [])\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*70)\n",
    "        print(\"PROMPT:\")\n",
    "        print(\"=\"*70)\n",
    "        print(prompt[:500] + \"...\" if len(prompt) > 500 else prompt)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=not greedy,\n",
    "            temperature=0.7 if not greedy else None,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(out[0][input_len:], skip_special_tokens=True)\n",
    "    thought, action = parse_output(generated)\n",
    "    is_valid = action is not None\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"OUTPUT:\")\n",
    "        print(\"=\"*70)\n",
    "        print(generated)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FORMAT CHECK:\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"  <think>:  {'✓' if '<think>' in generated else '❌'}\")\n",
    "        print(f\"  </think>: {'✓' if '</think>' in generated else '❌'}\")\n",
    "        print(f\"  Valid:    {'✓' if is_valid else '❌'}\")\n",
    "        if action:\n",
    "            print(f\"\\nACTION: {action}\")\n",
    "    \n",
    "    return {\"output\": generated, \"thought\": thought, \"action\": action, \"valid\": is_valid}\n",
    "\n",
    "print(\"✓ Generate function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1\n",
    "r1 = generate(\"Calculate the derivative of f(x) = x^3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: With previous step\n",
    "r2 = generate(\n",
    "    \"Solve: 2x + 5 = 13\",\n",
    "    previous_steps=[\"Step 1: Subtract 5 from both sides: 2x = 8\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3\n",
    "r3 = generate(\"What is 15% of 80?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Solve x^2 = 16\",\n",
    "    \"Find the area of a circle with radius 5\",\n",
    "    \"What is the derivative of sin(x)?\",\n",
    "    \"Factor x^2 - 9\",\n",
    "]\n",
    "\n",
    "print(\"Batch testing...\\n\")\n",
    "results = []\n",
    "for i, p in enumerate(problems, 1):\n",
    "    r = generate(p, verbose=False)\n",
    "    results.append(r)\n",
    "    s = \"✓\" if r[\"valid\"] else \"❌\"\n",
    "    print(f\"{s} Test {i}: {p[:40]}\")\n",
    "    if r[\"valid\"]:\n",
    "        print(f\"   → {r['action'][:60]}...\" if len(r['action']) > 60 else f\"   → {r['action']}\")\n",
    "    else:\n",
    "        print(f\"   → {r['output'][:60]}...\")\n",
    "\n",
    "valid = sum(r[\"valid\"] for r in results)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RESULT: {valid}/{len(results)} valid ({100*valid//len(results)}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
