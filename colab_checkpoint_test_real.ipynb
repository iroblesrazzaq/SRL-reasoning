{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3-4B-Instruct-2507 Base Model Format Testing\n",
    "\n",
    "Test whether the base model can follow the expected `<think>...</think>` format for SRL training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model (this may take a minute)...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on: {next(model.parameters()).device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Prompt Format and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "\n",
    "# SRL prompt preamble (from training)\n",
    "PROMPT_PREAMBLE = (\n",
    "    \"You are a helpful assistant for solving mathematical problems. \"\n",
    "    \"A user will provide a math problem, which may include a partial solution. \"\n",
    "    \"Your task is to continue the solution by providing the very next logical step. \"\n",
    "    \"A user will ask you to solve a task. You should first draft your thinking process \"\n",
    "    \"(inner monologue). Then, generate the solution. \"\n",
    "    \"Your response format must follow the template below:\\n\"\n",
    "    \"<think> Your thoughts or/and draft, like working through an exercise on scratch paper. \"\n",
    "    \"Be as casual and as long as you want until you are confident to generate a correct solution. </think>\\n\"\n",
    "    \"Provide only the single, next step to continue the solution. Do not solve the entire problem.\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_srl_prompt(\n",
    "    problem: str,\n",
    "    previous_steps: List[str] = None,\n",
    "    step_title: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Build the SRL prompt.\"\"\"\n",
    "    if previous_steps is None:\n",
    "        previous_steps = []\n",
    "    \n",
    "    parts = [\n",
    "        PROMPT_PREAMBLE,\n",
    "        \"\",\n",
    "        \"Problem:\",\n",
    "        problem.strip(),\n",
    "        \"\",\n",
    "    ]\n",
    "    for step in previous_steps:\n",
    "        parts.append(step.strip())\n",
    "    if step_title:\n",
    "        parts.append(step_title.strip())\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def parse_model_output(text: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Parse output to extract thought and action. Returns (None, None) if format invalid.\"\"\"\n",
    "    if text is None:\n",
    "        return (None, None)\n",
    "    \n",
    "    close_idx = text.find(\"</think>\")\n",
    "    if close_idx == -1:\n",
    "        return (None, None)  # Format error!\n",
    "    \n",
    "    action = text[close_idx + len(\"</think>\"):].strip()\n",
    "    \n",
    "    open_idx = text.find(\"<think>\")\n",
    "    if open_idx == -1:\n",
    "        thought = text[:close_idx].strip()\n",
    "    else:\n",
    "        thought = text[open_idx + len(\"<think>\"):close_idx].strip()\n",
    "    \n",
    "    return (thought, action)\n",
    "\n",
    "\n",
    "print(\"Functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(\n",
    "    problem: str,\n",
    "    previous_steps: List[str] = None,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7,\n",
    "    do_sample: bool = True,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"Generate a step and analyze output format.\"\"\"\n",
    "    if previous_steps is None:\n",
    "        previous_steps = []\n",
    "    \n",
    "    prompt = build_srl_prompt(problem, previous_steps)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"PROMPT:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(prompt)\n",
    "        print()\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature if do_sample else None,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"RAW OUTPUT:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(generated)\n",
    "        print()\n",
    "    \n",
    "    thought, action = parse_model_output(generated)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"FORMAT ANALYSIS:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        has_open = \"<think>\" in generated\n",
    "        has_close = \"</think>\" in generated\n",
    "        \n",
    "        print(f\"  Has <think>:  {has_open}\")\n",
    "        print(f\"  Has </think>: {has_close}\")\n",
    "        \n",
    "        if thought is None and action is None:\n",
    "            print(\"  ❌ FORMAT ERROR: Missing </think> tag!\")\n",
    "        else:\n",
    "            print(f\"  ✓ Valid format\")\n",
    "            print(f\"  Thought: {len(thought)} chars\")\n",
    "            print(f\"  Action:  {len(action)} chars\")\n",
    "        \n",
    "        if thought:\n",
    "            print(\"\\nTHOUGHT:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(thought[:500] + (\"...\" if len(thought) > 500 else \"\"))\n",
    "        \n",
    "        if action:\n",
    "            print(\"\\nACTION (step):\")\n",
    "            print(\"-\" * 40)\n",
    "            print(action)\n",
    "    \n",
    "    return {\n",
    "        \"raw_output\": generated,\n",
    "        \"thought\": thought,\n",
    "        \"action\": action,\n",
    "        \"is_valid\": action is not None,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Generation function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Base Model Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Simple problem\n",
    "result1 = generate_step(\n",
    "    problem=\"Calculate the derivative of f(x) = x^3 + 2x^2 - 5x + 1\",\n",
    "    do_sample=False,  # Greedy for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Problem with previous step\n",
    "result2 = generate_step(\n",
    "    problem=\"Solve the equation: 2x + 5 = 13\",\n",
    "    previous_steps=[\"Step 1: Subtract 5 from both sides: 2x + 5 - 5 = 13 - 5\"],\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Simple arithmetic\n",
    "result3 = generate_step(\n",
    "    problem=\"What is 15% of 80?\",\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Algebra\n",
    "result4 = generate_step(\n",
    "    problem=\"Simplify: 3(x + 4) - 2(x - 1)\",\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_problems = [\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Find the area of a circle with radius 5.\",\n",
    "    \"Solve x^2 = 16\",\n",
    "    \"What is the derivative of sin(x)?\",\n",
    "    \"Calculate 25 × 4\",\n",
    "]\n",
    "\n",
    "print(\"Running batch test with greedy decoding...\\n\")\n",
    "\n",
    "results = []\n",
    "for i, problem in enumerate(test_problems, 1):\n",
    "    print(f\"Test {i}: {problem[:50]}...\")\n",
    "    result = generate_step(problem, do_sample=False, verbose=False)\n",
    "    results.append(result)\n",
    "    status = \"✓\" if result[\"is_valid\"] else \"❌\"\n",
    "    print(f\"  {status} Valid: {result['is_valid']}\")\n",
    "    if not result[\"is_valid\"]:\n",
    "        print(f\"  Raw output preview: {result['raw_output'][:100]}...\")\n",
    "    print()\n",
    "\n",
    "# Summary\n",
    "valid = sum(1 for r in results if r[\"is_valid\"])\n",
    "print(\"=\" * 60)\n",
    "print(f\"SUMMARY: {valid}/{len(results)} valid format ({100*valid/len(results):.0f}%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test with Chat Template (Qwen's Native Format)\n",
    "\n",
    "Qwen3 Instruct models expect chat format. Let's try that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_chat_template(\n",
    "    problem: str,\n",
    "    previous_steps: List[str] = None,\n",
    "    max_new_tokens: int = 512,\n",
    "    do_sample: bool = False,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"Generate using Qwen's chat template.\"\"\"\n",
    "    if previous_steps is None:\n",
    "        previous_steps = []\n",
    "    \n",
    "    # Build system message\n",
    "    system_msg = (\n",
    "        \"You are a helpful assistant for solving mathematical problems. \"\n",
    "        \"Your task is to provide the very next logical step to continue a solution. \"\n",
    "        \"Your response format must follow this template:\\n\"\n",
    "        \"<think> Your thoughts or draft. </think>\\n\"\n",
    "        \"[Your single next step here]\\n\\n\"\n",
    "        \"Provide only ONE step. Do not solve the entire problem.\"\n",
    "    )\n",
    "    \n",
    "    # Build user message\n",
    "    user_content = f\"Problem: {problem}\"\n",
    "    if previous_steps:\n",
    "        user_content += \"\\n\\nPrevious steps:\\n\" + \"\\n\".join(previous_steps)\n",
    "    user_content += \"\\n\\nProvide the next step:\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"PROMPT (with chat template):\")\n",
    "        print(\"=\" * 80)\n",
    "        print(prompt)\n",
    "        print()\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"RAW OUTPUT:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(generated)\n",
    "        print()\n",
    "    \n",
    "    thought, action = parse_model_output(generated)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"FORMAT ANALYSIS:\")\n",
    "        print(\"=\" * 80)\n",
    "        has_open = \"<think>\" in generated\n",
    "        has_close = \"</think>\" in generated\n",
    "        print(f\"  Has <think>:  {has_open}\")\n",
    "        print(f\"  Has </think>: {has_close}\")\n",
    "        \n",
    "        if action is None:\n",
    "            print(\"  ❌ FORMAT ERROR\")\n",
    "        else:\n",
    "            print(f\"  ✓ Valid format\")\n",
    "            print(f\"  Action: {action[:200]}...\" if len(action) > 200 else f\"  Action: {action}\")\n",
    "    \n",
    "    return {\"raw_output\": generated, \"thought\": thought, \"action\": action, \"is_valid\": action is not None}\n",
    "\n",
    "\n",
    "print(\"Chat template function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with chat template\n",
    "result_chat = generate_with_chat_template(\n",
    "    problem=\"Calculate the derivative of f(x) = x^3 + 2x^2 - 5x + 1\",\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with previous step using chat template\n",
    "result_chat2 = generate_with_chat_template(\n",
    "    problem=\"Solve: 2x + 5 = 13\",\n",
    "    previous_steps=[\"Step 1: Subtract 5 from both sides: 2x = 8\"],\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Both Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_problems = [\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Find the area of a circle with radius 5.\",\n",
    "    \"Solve x^2 = 16\",\n",
    "]\n",
    "\n",
    "print(\"Comparing raw prompt vs chat template...\\n\")\n",
    "print(f\"{'Problem':<40} {'Raw Prompt':<12} {'Chat Template':<12}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for problem in test_problems:\n",
    "    r1 = generate_step(problem, do_sample=False, verbose=False)\n",
    "    r2 = generate_with_chat_template(problem, do_sample=False, verbose=False)\n",
    "    \n",
    "    s1 = \"✓\" if r1[\"is_valid\"] else \"❌\"\n",
    "    s2 = \"✓\" if r2[\"is_valid\"] else \"❌\"\n",
    "    \n",
    "    print(f\"{problem:<40} {s1:<12} {s2:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory if needed\n",
    "# import gc\n",
    "# del model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# print(\"Memory cleared.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
