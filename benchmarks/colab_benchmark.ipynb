{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI4tBLxvlVHh"
      },
      "source": [
        "# SRL Benchmarking (Colab)\n",
        "Run vLLM-based benchmarks for SRL/SFT models, saving results for later plotting."
      ],
      "id": "XI4tBLxvlVHh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHRzyR4slVHi"
      },
      "source": [
        "## 0. Setup\n",
        "- Runtime: GPU (A100 recommended)\n",
        "- HF token: set `HF_TOKEN` env var if needed for private models/datasets.\n",
        "- Repo: cloned into `/content/SRL-reasoning`."
      ],
      "id": "HHRzyR4slVHi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "d6d8Z87WlVHi"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Colab setup (run this first in a fresh runtime) ---\n",
        "REPO_URL = \"https://github.com/iroblesrazzaq/SRL-reasoning.git\"  # update if needed\n",
        "BRANCH = \"main\"                               # update if needed\n",
        "WORKDIR = \"/content/SRL-reasoning\"\n",
        "\n",
        "# Install before importing anything\n",
        "%pip install -U pip\n",
        "import os\n",
        "\n",
        "if not os.path.exists(WORKDIR):\n",
        "    !git clone --branch $BRANCH $REPO_URL $WORKDIR\n",
        "%cd $WORKDIR\n",
        "%pip install -e .\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Ready. Repo at\", WORKDIR)"
      ],
      "id": "d6d8Z87WlVHi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Imnpey0lVHi"
      },
      "source": [
        "## 1. (Optional) Mount Drive for saving results/checkpoints"
      ],
      "id": "6Imnpey0lVHi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQvwFKEglVHi"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "MOUNT_DRIVE = True  # set True to mount\n",
        "if MOUNT_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_RESULTS_DIR = '/content/drive/MyDrive/srl_bench_results'\n",
        "else:\n",
        "    DRIVE_RESULTS_DIR = None\n"
      ],
      "id": "uQvwFKEglVHi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO4RChBklVHj"
      },
      "source": [
        "## 2. Configure run"
      ],
      "id": "TO4RChBklVHj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnIHCLlDlVHj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Required: set your fine-tuned model location (HF Hub id or local path)\n",
        "MODEL_PATH = \"Qwen/Qwen3-4B-Instruct-2507\"  # e.g., \"yourname/srl-lora\" or \"/content/ckpts/best\"\n",
        "MODEL_NAME = \"Qwen3-4B-Instruct-2507\"  # optional display name; defaults to basename of MODEL_PATH\n",
        "\n",
        "# Benchmark settings\n",
        "BENCHMARK = \"aime24\"          # choices: amc23, aime24, aime25\n",
        "MODE = \"avg32\"                # choices: greedy, avg32\n",
        "MODEL_TYPE = \"base\"            # choices: srl (has <think>), base (no <think>)\n",
        "GPU_MEMORY_UTILIZATION = 0.7  # leaves headroom alongside vLLM\n",
        "RESULTS_DIR = \"benchmarks/results\"\n",
        "\n",
        "SEED = 42"
      ],
      "id": "tnIHCLlDlVHj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjfCNRPTlVHj"
      },
      "source": [
        "## 3. Run benchmark and save result"
      ],
      "id": "PjfCNRPTlVHj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lstwQut8lVHj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import time\n",
        "from pathlib import Path\n",
        "from benchmarks import load_benchmark_data, MathEvaluator, BenchmarkResult, data_loader\n",
        "import os as _os\n",
        "\n",
        "_gpu_util = str(GPU_MEMORY_UTILIZATION)\n",
        "_os.environ.setdefault(\"VLLM_GPU_MEMORY_UTILIZATION\", _gpu_util)\n",
        "\n",
        "data_loader.BENCHMARK_CONFIGS[\"aime25\"] = (\"math-ai/aime25\", \"test\")\n",
        "\n",
        "print(f\"Loading benchmark '{BENCHMARK}'...\")\n",
        "data = load_benchmark_data(BENCHMARK)\n",
        "print(f\"Loaded {len(data)} problems\")\n",
        "\n",
        "print(\"Initializing evaluator (vLLM)...\")\n",
        "eval_start = time.time()\n",
        "evaluator = MathEvaluator(\n",
        "    MODEL_PATH,\n",
        "    model_type=MODEL_TYPE,\n",
        "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION\n",
        ")\n",
        "\n"
      ],
      "id": "lstwQut8lVHj"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Running mode='{MODE}' ...\")\n",
        "score = evaluator.evaluate(data, mode=MODE)\n",
        "eval_end = time.time() - eval_start\n",
        "print(f\"Score: {score:.4f} | elapsed: {eval_end/60:.1f} min\")\n",
        "\n",
        "benchmark_type = \"Avg32\" if MODE == \"avg32\" else \"Greedy\"\n",
        "model_display = MODEL_NAME or Path(MODEL_PATH).name\n",
        "\n",
        "result = BenchmarkResult(\n",
        "    benchmark=BENCHMARK,\n",
        "    benchmark_type=benchmark_type,\n",
        "    score=score,\n",
        "    model_name=model_display,\n",
        "    model_path=MODEL_PATH,\n",
        "    num_questions=len(data),\n",
        "    eval_time_seconds=eval_end,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "path = result.save(RESULTS_DIR)\n",
        "print(\"Saved result to\", path)\n"
      ],
      "metadata": {
        "id": "FXIftSHH3asp"
      },
      "id": "FXIftSHH3asp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODE = 'greedy'\n",
        "print(f\"Running mode='{MODE}' ...\")\n",
        "score = evaluator.evaluate(data, mode=MODE)\n",
        "eval_end = time.time() - eval_start\n",
        "print(f\"Score: {score:.4f} | elapsed: {eval_end/60:.1f} min\")\n",
        "\n",
        "benchmark_type = \"Avg32\" if MODE == \"avg32\" else \"Greedy\"\n",
        "model_display = MODEL_NAME or Path(MODEL_PATH).name\n",
        "\n",
        "result = BenchmarkResult(\n",
        "    benchmark=BENCHMARK,\n",
        "    benchmark_type=benchmark_type,\n",
        "    score=score,\n",
        "    model_name=model_display,\n",
        "    model_path=MODEL_PATH,\n",
        "    num_questions=len(data),\n",
        "    eval_time_seconds=eval_end,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "path = result.save(RESULTS_DIR)\n",
        "print(\"Saved result to\", path)\n"
      ],
      "metadata": {
        "id": "zqMQwaX5_0U9"
      },
      "id": "zqMQwaX5_0U9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint save in case amc23 crashes:\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "if DRIVE_RESULTS_DIR:\n",
        "    dest = Path(DRIVE_RESULTS_DIR)\n",
        "    dest.mkdir(parents=True, exist_ok=True)\n",
        "    for f in Path(RESULTS_DIR).glob(\"*.json\"):\n",
        "        shutil.copy2(f, dest / f.name)\n",
        "    print(\"Copied results to\", dest)\n",
        "else:\n",
        "    print(\"Drive not mounted; skipping copy.\")\n"
      ],
      "metadata": {
        "id": "zY9hZh2EAY34"
      },
      "id": "zY9hZh2EAY34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BENCHMARK = \"amc23\"          # choices: amc23, aime24, aime25\n",
        "MODE = \"avg32\"                # choices: greedy, avg32\n",
        "\n",
        "data = load_benchmark_data(BENCHMARK)\n",
        "\n",
        "print(f\"Running mode='{MODE}' ...\")\n",
        "score = evaluator.evaluate(data, mode=MODE)\n",
        "eval_end = time.time() - eval_start\n",
        "print(f\"Score: {score:.4f} | elapsed: {eval_end/60:.1f} min\")\n",
        "\n",
        "benchmark_type = \"Avg32\" if MODE == \"avg32\" else \"Greedy\"\n",
        "model_display = MODEL_NAME or Path(MODEL_PATH).name\n",
        "\n",
        "result = BenchmarkResult(\n",
        "    benchmark=BENCHMARK,\n",
        "    benchmark_type=benchmark_type,\n",
        "    score=score,\n",
        "    model_name=model_display,\n",
        "    model_path=MODEL_PATH,\n",
        "    num_questions=len(data),\n",
        "    eval_time_seconds=eval_end,\n",
        "    seed=SEED,\n",
        "\n",
        ")\n",
        "\n",
        "path = result.save(RESULTS_DIR)\n",
        "print(\"Saved result to\", path)\n",
        "\n"
      ],
      "metadata": {
        "id": "D_M9lh43_-el"
      },
      "id": "D_M9lh43_-el",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BENCHMARK = \"aime24\"          # choices: amc23, aime24, aime25\n",
        "data = load_benchmark_data(BENCHMARK)\n",
        "\n",
        "MODE = \"greedy\"                # choices: greedy, avg32\n",
        "\n",
        "print(f\"Running mode='{MODE}' ...\")\n",
        "score = evaluator.evaluate(data, mode=MODE)\n",
        "eval_end = time.time() - eval_start\n",
        "print(f\"Score: {score:.4f} | elapsed: {eval_end/60:.1f} min\")\n",
        "\n",
        "benchmark_type = \"Avg32\" if MODE == \"avg32\" else \"Greedy\"\n",
        "model_display = MODEL_NAME or Path(MODEL_PATH).name\n",
        "\n",
        "result = BenchmarkResult(\n",
        "    benchmark=BENCHMARK,\n",
        "    benchmark_type=benchmark_type,\n",
        "    score=score,\n",
        "    model_name=model_display,\n",
        "    model_path=MODEL_PATH,\n",
        "    num_questions=len(data),\n",
        "    eval_time_seconds=eval_end,\n",
        "    seed=SEED,\n",
        "\n",
        ")\n",
        "\n",
        "path = result.save(RESULTS_DIR)\n",
        "print(\"Saved result to\", path)\n",
        "\n"
      ],
      "metadata": {
        "id": "NAXsqBueApgc"
      },
      "id": "NAXsqBueApgc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3J6NpsvlVHj"
      },
      "source": [
        "## 4. (Optional) Copy results to Drive"
      ],
      "id": "U3J6NpsvlVHj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEvwy9TVlVHj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "if DRIVE_RESULTS_DIR:\n",
        "    dest = Path(DRIVE_RESULTS_DIR)\n",
        "    dest.mkdir(parents=True, exist_ok=True)\n",
        "    for f in Path(RESULTS_DIR).glob(\"*.json\"):\n",
        "        shutil.copy2(f, dest / f.name)\n",
        "    print(\"Copied results to\", dest)\n",
        "else:\n",
        "    print(\"Drive not mounted; skipping copy.\")\n"
      ],
      "id": "OEvwy9TVlVHj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUW49yDXlVHj"
      },
      "source": [
        "## 5. Inspect saved results"
      ],
      "id": "cUW49yDXlVHj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSC47HiTlVHj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from benchmarks import load_all_results, summarize_results\n",
        "\n",
        "results = list(load_all_results(RESULTS_DIR))\n",
        "print(f\"Loaded {len(results)} result file(s)\")\n",
        "for r in results:\n",
        "    print(f\"- {r.benchmark} | {r.benchmark_type} | {r.model_name} | score={r.score:.4f} | run_id={r.run_id}\")\n",
        "\n",
        "best = summarize_results(results)\n",
        "print(\"Best scores (benchmark, model_name) -> score:\")\n",
        "for (bench, model), sc in best.items():\n",
        "    print(f\"{bench} / {model}: {sc:.4f}\")\n"
      ],
      "id": "sSC47HiTlVHj"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LVk5H9qrqLSH"
      },
      "id": "LVk5H9qrqLSH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}