{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SRL Benchmarking (Colab)\n",
        "Run vLLM-based benchmarks for SRL/SFT models, saving results for later plotting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup\n",
        "- Runtime: GPU (A100 recommended)\n",
        "- HF token: set `HF_TOKEN` env var if needed for private models/datasets.\n",
        "- Repo: cloned into `/content/SRL-reasoning`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import os, subprocess, sys\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = \"https://github.com/your-org/SRL-reasoning.git\"  # update if different\n",
        "BRANCH = \"assistant-work-fix\"  # change if you want another branch\n",
        "WORKDIR = \"/content/SRL-reasoning\"\n",
        "\n",
        "# Clone repo if missing\n",
        "if not os.path.exists(WORKDIR):\n",
        "    !git clone --branch $BRANCH $REPO_URL $WORKDIR\n",
        "else:\n",
        "    print(\"Repo already present at\", WORKDIR)\n",
        "\n",
        "%cd $WORKDIR\n",
        "\n",
        "# Install in editable mode (ensures imports work without sys.path hacks)\n",
        "!pip install -U pip\n",
        "!pip install -e .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. (Optional) Mount Drive for saving results/checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "MOUNT_DRIVE = False  # set True to mount\n",
        "if MOUNT_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_RESULTS_DIR = '/content/drive/MyDrive/srl_bench_results'\n",
        "else:\n",
        "    DRIVE_RESULTS_DIR = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Required: set your fine-tuned model location (HF Hub id or local path)\n",
        "MODEL_PATH = \"path-or-hub-id\"  # e.g., \"yourname/srl-lora\" or \"/content/ckpts/best\"\n",
        "MODEL_NAME = None  # optional display name; defaults to basename of MODEL_PATH\n",
        "\n",
        "# Benchmark settings\n",
        "BENCHMARK = \"aime25\"          # choices: amc23, aime24, aime25\n",
        "MODE = \"avg32\"                # choices: greedy, avg32\n",
        "MODEL_TYPE = \"srl\"            # choices: srl (has <think>), base (no <think>)\n",
        "GPU_MEMORY_UTILIZATION = 0.4  # leaves headroom alongside vLLM\n",
        "RESULTS_DIR = \"benchmarks/results\"\n",
        "\n",
        "# Repro / logging\n",
        "SEED = 42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run benchmark and save result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import time\n",
        "from pathlib import Path\n",
        "from benchmarks import load_benchmark_data, MathEvaluator, BenchmarkResult\n",
        "import os as _os\n",
        "\n",
        "# Set env vars before vLLM init\n",
        "_os.environ.setdefault(\"VLLM_GPU_MEMORY_UTILIZATION\", str(GPU_MEMORY_UTILIZATION))\n",
        "\n",
        "# Load data\n",
        "print(f\"Loading benchmark '{BENCHMARK}'...\")\n",
        "data = load_benchmark_data(BENCHMARK)\n",
        "print(f\"Loaded {len(data)} problems\")\n",
        "\n",
        "# Init evaluator\n",
        "print(\"Initializing evaluator (vLLM)...\")\n",
        "eval_start = time.time()\n",
        "evaluator = MathEvaluator(MODEL_PATH, model_type=MODEL_TYPE, gpu_memory_utilization=GPU_MEMORY_UTILIZATION)\n",
        "\n",
        "# Run\n",
        "print(f\"Running mode={MODE} ...\")\n",
        "score = evaluator.evaluate(data, mode=MODE)\n",
        "elapsed = time.time() - eval_start\n",
        "print(f\"Score: {score:.4f}  | elapsed: {elapsed/60:.1f} min\")\n",
        "\n",
        "benchmark_type = \"Avg@32\" if MODE == \"avg32\" else \"Greedy\"\n",
        "model_display = MODEL_NAME or Path(MODEL_PATH).name\n",
        "result = BenchmarkResult(\n",
        "    benchmark=BENCHMARK,\n",
        "    benchmark_type=benchmark_type,\n",
        "    score=score,\n",
        "    model_name=model_display,\n",
        "    model_path=MODEL_PATH,\n",
        "    num_questions=len(data),\n",
        "    eval_time_seconds=elapsed,\n",
        "    seed=SEED,\n",
        "    extra={\"mode\": MODE, \"model_type\": MODEL_TYPE},\n",
        ")\n",
        "path = result.save(RESULTS_DIR)\n",
        "print(\"Saved result to\", path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. (Optional) Copy results to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "if DRIVE_RESULTS_DIR:\n",
        "    dest = Path(DRIVE_RESULTS_DIR)\n",
        "    dest.mkdir(parents=True, exist_ok=True)\n",
        "    for f in Path(RESULTS_DIR).glob(\"*.json\"):\n",
        "        shutil.copy2(f, dest / f.name)\n",
        "    print(\"Copied results to\", dest)\n",
        "else:\n",
        "    print(\"Drive not mounted; skipping copy.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Inspect saved results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from benchmarks import load_all_results, summarize_results\n",
        "\n",
        "results = list(load_all_results(RESULTS_DIR))\n",
        "print(f\"Loaded {len(results)} result file(s)\")\n",
        "for r in results:\n",
        "    print(f\"- {r.benchmark} | {r.benchmark_type} | {r.model_name} | score={r.score:.4f} | run_id={r.run_id}\")\n",
        "\n",
        "best = summarize_results(results)\n",
        "print(\"\n",
        "Best scores (benchmark, model_name) -> score:\")\n",
        "for (bench, model), sc in best.items():\n",
        "    print(f\"{bench} / {model}: {sc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}