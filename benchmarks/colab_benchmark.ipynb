{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "XI4tBLxvlVHh",
      "metadata": {
        "id": "XI4tBLxvlVHh"
      },
      "source": [
        "# SRL Benchmarking (Colab)\n",
        "Run vLLM-based benchmarks for SRL/SFT models, saving results for later plotting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HHRzyR4slVHi",
      "metadata": {
        "id": "HHRzyR4slVHi"
      },
      "source": [
        "## 0. Setup\n",
        "- Runtime: GPU (A100 recommended)\n",
        "- HF token: set `HF_TOKEN` env var if needed for private models/datasets.\n",
        "- Repo: cloned into `/content/SRL-reasoning`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6d8Z87WlVHi",
      "metadata": {
        "collapsed": true,
        "id": "d6d8Z87WlVHi"
      },
      "outputs": [],
      "source": [
        "# --- Colab setup (run this first in a fresh runtime) ---\n",
        "REPO_URL = \"https://github.com/iroblesrazzaq/SRL-reasoning.git\"  # update if needed\n",
        "BRANCH = \"main\"                               # update if needed\n",
        "WORKDIR = \"/content/SRL-reasoning\"\n",
        "\n",
        "# Install before importing anything\n",
        "%pip install -U pip\n",
        "import os\n",
        "\n",
        "if not os.path.exists(WORKDIR):\n",
        "    !git clone --branch $BRANCH $REPO_URL $WORKDIR\n",
        "%cd $WORKDIR\n",
        "%pip install -e .\n",
        "\n",
        "from pathlib import Path\n",
        "import time\n",
        "from benchmarks import load_benchmark_data, MathEvaluator, BenchmarkResult, data_loader\n",
        "import os as _os\n",
        "import shutil\n",
        "\n",
        "\n",
        "print(\"Ready. Repo at\", WORKDIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6Imnpey0lVHi",
      "metadata": {
        "id": "6Imnpey0lVHi"
      },
      "source": [
        "## 1. (Optional) Mount Drive for saving results/checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uQvwFKEglVHi",
      "metadata": {
        "id": "uQvwFKEglVHi"
      },
      "outputs": [],
      "source": [
        "\n",
        "MOUNT_DRIVE = True  # set True to mount\n",
        "if MOUNT_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_RESULTS_DIR = '/content/drive/MyDrive/srl_bench_results'\n",
        "else:\n",
        "    DRIVE_RESULTS_DIR = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TO4RChBklVHj",
      "metadata": {
        "id": "TO4RChBklVHj"
      },
      "source": [
        "## 2. Configs - for fine-tuned model, must set path to local path (mount from drive?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tnIHCLlDlVHj",
      "metadata": {
        "id": "tnIHCLlDlVHj"
      },
      "outputs": [],
      "source": [
        "\n",
        "MODEL_PATH = \"Qwen/Qwen3-4B-Instruct-2507\"  # e.g., \"yourname/srl-lora\" or \"/content/ckpts/best\"\n",
        "MODEL_NAME = \"Qwen3-4B-Instruct-2507\"  # optional display name; defaults to basename of MODEL_PATH\n",
        "\n",
        "# TODO: load trained SFT/SRL models, name them as f\"{MODEL_NAME}-sft\" or f\"{MODEL_NAME}-srl\"\n",
        "\n",
        "MODEL_TYPE = \"base\"            # choices: srl (has <think>), base (no <think>) TODO: select srl if using SRL model\n",
        "GPU_MEMORY_UTILIZATION = 0.8  \n",
        "RESULTS_DIR = \"benchmarks/results\"\n",
        "\n",
        "SEED = 42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PjfCNRPTlVHj",
      "metadata": {
        "id": "PjfCNRPTlVHj"
      },
      "source": [
        "## 3. Instantiate evaluator (vLLM allocates memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lstwQut8lVHj",
      "metadata": {
        "id": "lstwQut8lVHj"
      },
      "outputs": [],
      "source": [
        "_gpu_util = str(GPU_MEMORY_UTILIZATION)\n",
        "_os.environ.setdefault(\"VLLM_GPU_MEMORY_UTILIZATION\", _gpu_util)\n",
        "\n",
        "evaluator = MathEvaluator(\n",
        "    MODEL_PATH,\n",
        "    model_type=MODEL_TYPE,\n",
        "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49c8e5ef",
      "metadata": {},
      "source": [
        "### 4. benchmarking loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bdb23e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "BENCHMARKS = [\"aime24\", \"aime25\", 'amc23']          \n",
        "MODES = [\"greedy\", \"avg32\"]   \n",
        "MODES = [\"greedy\"] # for testing\n",
        "\n",
        "for benchmark in BENCHMARKS:\n",
        "    for mode in MODES:\n",
        "        eval_start = time.time()\n",
        "        data = load_benchmark_data(benchmark)\n",
        "        #print(f\"Running mode='{mode}' for benchmark='{benchmark}' ...\")\n",
        "        score = evaluator.evaluate(data, mode=mode)\n",
        "        eval_end = time.time() - eval_start\n",
        "        #print(f\"Score: {score:.4f} | elapsed: {eval_end/60:.1f} min\")\n",
        "        benchmark_type = \"Avg32\" if mode == \"avg32\" else \"Greedy\"\n",
        "\n",
        "        model_display = MODEL_NAME or Path(MODEL_PATH).name\n",
        "\n",
        "        result = BenchmarkResult(\n",
        "            benchmark=benchmark,\n",
        "            benchmark_type=benchmark_type,\n",
        "            score=score,\n",
        "            model_name=model_display,\n",
        "            model_path=MODEL_PATH,\n",
        "            num_questions=len(data),\n",
        "            eval_time_seconds=eval_end,\n",
        "            seed=SEED,\n",
        "        )\n",
        "\n",
        "        path = result.save(RESULTS_DIR)\n",
        "        print(\"Saved result to\", path)\n",
        "\n",
        "        # save to drive every benchmark in case of crash\n",
        "        if DRIVE_RESULTS_DIR:\n",
        "            dest = Path(DRIVE_RESULTS_DIR)\n",
        "            dest.mkdir(parents=True, exist_ok=True)\n",
        "            for f in Path(RESULTS_DIR).glob(\"*.json\"):\n",
        "                shutil.copy2(f, dest / f.name)\n",
        "            print(\"Copied results to\", dest)\n",
        "        else:\n",
        "            print(\"Drive not mounted; skipping copy.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47b99bc4",
      "metadata": {},
      "source": [
        "### 4. benchmark over all datasets, modes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cUW49yDXlVHj",
      "metadata": {
        "id": "cUW49yDXlVHj"
      },
      "source": [
        "## 5. Inspect saved results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sSC47HiTlVHj",
      "metadata": {
        "id": "sSC47HiTlVHj"
      },
      "outputs": [],
      "source": [
        "\n",
        "from benchmarks import load_all_results, summarize_results\n",
        "\n",
        "results = list(load_all_results(RESULTS_DIR))\n",
        "print(f\"Loaded {len(results)} result file(s)\")\n",
        "for r in results:\n",
        "    print(f\"- {r.benchmark} | {r.benchmark_type} | {r.model_name} | score={r.score:.4f} | run_id={r.run_id}\")\n",
        "\n",
        "best = summarize_results(results)\n",
        "print(\"Best scores (benchmark, model_name) -> score:\")\n",
        "for (bench, model), sc in best.items():\n",
        "    print(f\"{bench} / {model}: {sc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
