{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt Preview (Local)\n",
        "Preview the exact prompts used for SRL/base benchmarking by sampling one problem from a benchmark dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage\n",
        "- Run this notebook from the repo root (no Colab clone steps).\n",
        "- Ensure dependencies are installed (`pip install -e .`).\n",
        "- Choose a benchmark and model type below and run the cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca9b992",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure repo root is on sys.path for local runs\n",
        "import sys\n",
        "from pathlib import Path\n",
        "ROOT = Path.cwd()\n",
        "while ROOT != ROOT.parent and not (ROOT / 'pyproject.toml').exists():\n",
        "    ROOT = ROOT.parent\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "print('Using project root:', ROOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16ffb726",
      "metadata": {},
      "source": [
        "## Configure benchmark and model type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c031c680",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark: amc23, aime24, or aime25\n",
        "BENCHMARK = \"aime25\"\n",
        "MODEL_TYPE = \"base\"  # \"srl\" (with <think>) or \"base\" (no <think>)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "000fae7d",
      "metadata": {},
      "source": [
        "## Load one example and build the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41def027",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "from benchmarks.data_loader import load_benchmark_data\n",
        "\n",
        "# Load data (first example only)\n",
        "data = load_benchmark_data(BENCHMARK)\n",
        "example = data[0]\n",
        "print(f\"Loaded {len(data)} problems; showing first one.\\n\")\n",
        "\n",
        "# Extract templates directly from evaluator source (avoids importing vllm)\n",
        "text = (ROOT / \"benchmarks\" / \"evaluator.py\").read_text()\n",
        "base_match = re.search(r'BASE_PROMPT_TEMPLATE = \"\"\"(.*?)\"\"\"', text, re.S)\n",
        "srl_match = re.search(r'SRL_PROMPT_TEMPLATE = \"\"\"(.*?)\"\"\"', text, re.S)\n",
        "base_template = base_match.group(1) if base_match else None\n",
        "srl_template = srl_match.group(1) if srl_match else None\n",
        "\n",
        "if MODEL_TYPE == \"srl\":\n",
        "    from src.shared.prompts import build_srl_prompt\n",
        "    prompt = build_srl_prompt(example[\"problem\"], [], include_closing_tag=False)\n",
        "    print(\"SRL prompt template from evaluator:\\n\")\n",
        "    print(srl_template)\n",
        "else:\n",
        "    # Avoid Python .format interpreting the \\boxed{} braces; do a simple replacement\n",
        "    prompt = base_template.replace(\"{problem}\", example[\"problem\"])\n",
        "    print(\"Base prompt template from evaluator:\\n\")\n",
        "    print(base_template)\n",
        "\n",
        "print(\"\\n---\\nFull prompt with sample problem:\\n\")\n",
        "print(prompt)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
